package com.lamontlabs.quantravision.ai.ensemble

import android.content.Context
import com.lamontlabs.quantravision.ai.ensemble.knowledge.QAKnowledgeBase
import com.lamontlabs.quantravision.devbot.diagnostics.ComponentHealthMonitor
import com.lamontlabs.quantravision.devbot.diagnostics.HealthStatus
import com.lamontlabs.quantravision.devbot.diagnostics.ModelDiagnostics
import com.lamontlabs.quantravision.devbot.diagnostics.StartupDiagnosticCollector
import com.lamontlabs.quantravision.devbot.diagnostics.StartupStatus
import com.lamontlabs.quantravision.intelligence.llm.ExplanationResult
import com.lamontlabs.quantravision.intelligence.llm.InitializationError
import com.lamontlabs.quantravision.intelligence.llm.ModelManager
import com.lamontlabs.quantravision.intelligence.llm.ModelState
import com.lamontlabs.quantravision.intelligence.llm.ModelType
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.sync.Mutex
import kotlinx.coroutines.sync.withLock
import kotlinx.coroutines.withContext
import timber.log.Timber

/**
 * Ensemble AI Engine orchestrating 2-3 TFLite models for fast, efficient Q&A
 * 
 * # ARCHITECTURE
 * 
 * This engine replaces the 555MB Gemma model with lightweight TFLite models:
 * 
 * **Required models (2):**
 * 1. **EmbeddingsRetriever** (25MB) - Semantic similarity search over 198 Q&A pairs
 * 2. **MobileBERT Q&A** (110MB) - Extractive question answering for novel questions
 * 
 * **Optional model (1):**
 * 3. **IntentClassifier** (15MB) - Classifies user intent (pattern_explanation, quantra_score, etc.)
 * 
 * Total size: ~135MB required (150MB with optional intent classifier, vs 555MB Gemma)
 * 
 * # INTERFACE COMPATIBILITY
 * 
 * Maintains **exact same interface** as GemmaEngine:
 * - Singleton pattern: getInstance(context)
 * - initialize(): Result<Unit>
 * - generate(prompt: String, maxTokens: Int): ExplanationResult
 * - isReady(): Boolean
 * - getState(): ModelState
 * 
 * # ORCHESTRATION FLOW
 * 
 * ```
 * generate(prompt) â†’
 *   1. Check if required models loaded (if not, return ExplanationResult.Unavailable)
 *   2. [OPTIONAL] Classify intent using IntentClassifier (if available)
 *   3. Search for answer using EmbeddingsRetriever
 *   4. If retrieval confidence > 0.75:
 *      Return ExplanationResult.Success(text = retrievedAnswer, fromCache = true)
 *   5. Else fall back to MobileBERT:
 *      Build context from prompt + pattern info
 *      Get answer from MobileBERTQaAdapter
 *      Return ExplanationResult.Success(text = generatedAnswer, fromCache = false)
 *   6. Handle all errors gracefully (return ExplanationResult.Failure or Unavailable)
 * ```
 * 
 * # STATE MANAGEMENT
 * 
 * This engine truthfully reports model availability through ModelState:
 * - **Ready**: Required models (embeddings + MobileBERT) loaded and ready for inference
 * - **Downloaded**: Required model files exist but not loaded into memory
 * - **PartiallyDownloaded**: Missing required models (cannot initialize)
 * - **NotDownloaded**: No model files present
 * 
 * ## Initialization Contract (CRITICAL FOR VALIDATION)
 * 
 * ### initialize() returns Result.failure when:
 * - Required model files don't exist (enables validation workflows to detect missing models)
 * - Model files exist but initialization fails
 * - Any error occurs during initialization
 * 
 * ### initialize() returns Result.success ONLY when:
 * - Required models (embeddings + MobileBERT) successfully loaded and ready for inference
 * - Intent classifier is optional and will be loaded if available
 * 
 * ## Behavior by Model Availability
 * 
 * ### When NO model files exist:
 * - `initialize()` **fails** with InitializationError.ModelNotFound
 * - `generate()` returns `ExplanationResult.Unavailable` with fallback text
 * - `isReady()` returns `false`
 * - PatternExplainer uses FallbackExplanations for template-based responses
 * 
 * ### When missing required models (PartiallyDownloaded):
 * - `initialize()` **fails** with InitializationError.PartialModels
 * - User sees clear error message indicating which required models are missing
 * - `generate()` returns `ExplanationResult.Unavailable`
 * 
 * ### When required models loaded (Ready):
 * - `generate()` uses fast retrieval-first approach (10x faster than generation)
 * - High-confidence matches (>0.75) return instantly from cache
 * - Novel questions fall back to MobileBERT generation
 * - `isReady()` returns `true`
 * - Intent classification is used if intent classifier is loaded, otherwise skipped
 * 
 * # PERFORMANCE BENEFITS
 * 
 * - **10x faster** for common questions (embedding retrieval vs LLM generation)
 * - **4.1x smaller** model size (135MB required vs 555MB Gemma)
 * - **Lower memory** usage (no need to load 555MB model)
 * - **Same user experience** (drop-in replacement for GemmaEngine)
 * - **Works with just 2 models** (135MB) - intent classifier optional
 * 
 * @see IntentClassifier for optional intent classification
 * @see EmbeddingsRetriever for semantic similarity search
 * @see MobileBERTQaAdapter for extractive Q&A
 */
class EnsembleEngine private constructor(private val context: Context) {
    
    private val modelManager = ModelManager(context)
    private val knowledgeBase = QAKnowledgeBase(context)
    
    private var modelState: ModelState = ModelState.NotDownloaded
    
    // 3 ensemble components (initialized when models are loaded)
    @Volatile
    private var intentClassifier: IntentClassifier? = null
    
    @Volatile
    private var embeddingsRetriever: EmbeddingsRetriever? = null
    
    @Volatile
    private var mobileBertQa: MobileBERTQaAdapter? = null
    
    // Mutex for thread-safe initialization
    private val initMutex = Mutex()
    
    companion object {
        private const val TAG = "EnsembleEngine"
        private const val RETRIEVAL_CONFIDENCE_THRESHOLD = 0.75f
        
        /**
         * Shared instance for memory efficiency
         * Both DevBot and QuantraBot should share ONE EnsembleEngine instance
         */
        @Volatile
        private var sharedInstance: EnsembleEngine? = null
        
        /**
         * Get or create shared EnsembleEngine instance
         * 
         * This ensures only ONE set of models is loaded in memory, even when used by
         * both DevBot and QuantraBot simultaneously.
         */
        @Synchronized
        fun getInstance(context: Context): EnsembleEngine {
            return sharedInstance ?: EnsembleEngine(context.applicationContext).also {
                sharedInstance = it
            }
        }
    }
    
    init {
        modelState = modelManager.getModelState()
        Timber.i("ðŸŽ¯ EnsembleEngine initialized - State: $modelState [BUILD: 2025-11-20-MOBILEBERT-OPTIONAL-v2]")
    }
    
    /**
     * Initialize ensemble models (2 required, 1 optional)
     * 
     * ## Initialization Contract
     * 
     * Returns **Result.failure** when:
     * - Required model files don't exist (enables validation workflows to detect missing models)
     * - Model files exist but initialization fails
     * - Any error occurs during initialization
     * 
     * Returns **Result.success** ONLY when:
     * - Required models (embeddings + MobileBERT) successfully loaded and ready for inference
     * - Intent classifier loaded if available, otherwise skipped with logging
     * 
     * ## State Transitions
     * 
     * - No model files: Sets state to NotDownloaded, returns Result.failure
     * - Missing required models: Sets state to PartiallyDownloaded, returns Result.failure
     * - Required models exist: Sets state to Loading, attempts initialization
     * - Initialization succeeds: Sets state to Ready, returns Result.success
     * - Initialization fails: Sets state to Error, returns Result.failure
     */
    suspend fun initialize(): Result<Unit> = withContext(Dispatchers.IO) {
        initMutex.withLock {
            try {
                StartupDiagnosticCollector.logEvent(
                    component = "Ensemble AI Engine",
                    event = "Starting initialization",
                    status = StartupStatus.STARTED
                )
                
                // If already ready, return success (idempotent)
                if (modelState is ModelState.Ready && 
                    embeddingsRetriever != null && 
                    mobileBertQa != null) {
                    Timber.d("ðŸŽ¯ Ensemble already initialized and ready")
                    StartupDiagnosticCollector.logEvent(
                        component = "Ensemble AI Engine",
                        event = "Already initialized",
                        status = StartupStatus.SUCCESS
                    )
                    ComponentHealthMonitor.updateComponentHealth(
                        componentName = "Ensemble AI Engine",
                        status = HealthStatus.HEALTHY,
                        message = "All models loaded"
                    )
                    return@withContext Result.success(Unit)
                }
                
                // Refresh model state from manager
                modelState = modelManager.getModelState()
                
                when (val state = modelState) {
                    is ModelState.NotDownloaded -> {
                        Timber.w("ðŸŽ¯ No ensemble models found")
                        val errorMsg = "No ensemble model files downloaded. See ENSEMBLE_MODEL_DOWNLOADS.md"
                        StartupDiagnosticCollector.logEvent(
                            component = "Ensemble AI Engine",
                            event = "Initialization failed",
                            status = StartupStatus.FAILED,
                            error = errorMsg
                        )
                        ComponentHealthMonitor.updateComponentHealth(
                            componentName = "Ensemble AI Engine",
                            status = HealthStatus.FAILED,
                            message = errorMsg
                        )
                        return@withContext Result.failure(
                            InitializationError.ModelNotFound(errorMsg)
                        )
                    }
                    
                    is ModelState.PartiallyDownloaded -> {
                        val missing = getMissingModels(state)
                        val errorMsg = "Only ${state.importedCount}/${state.totalRequired} required models imported. Missing: $missing"
                        Timber.w("ðŸŽ¯ $errorMsg")
                        StartupDiagnosticCollector.logEvent(
                            component = "Ensemble AI Engine",
                            event = "Initialization failed",
                            status = StartupStatus.FAILED,
                            error = errorMsg
                        )
                        ComponentHealthMonitor.updateComponentHealth(
                            componentName = "Ensemble AI Engine",
                            status = HealthStatus.FAILED,
                            message = errorMsg
                        )
                        return@withContext Result.failure(
                            InitializationError.PartialModels(
                                imported = state.importedCount,
                                total = state.totalRequired,
                                missing = missing
                            )
                        )
                    }
                    
                    is ModelState.Downloaded -> {
                        Timber.i("ðŸŽ¯ Loading ensemble models...")
                        modelState = ModelState.Loading
                        
                        try {
                            // Get required model files
                            val embeddingsFile = modelManager.getEmbeddingsModelFile()
                            val mobileBertFile = modelManager.getMobileBertFile()
                            
                            if (embeddingsFile == null) {
                                val errorMsg = "Embeddings model file missing despite Downloaded state"
                                Timber.e("ðŸŽ¯ $errorMsg")
                                StartupDiagnosticCollector.logEvent(
                                    component = "Ensemble AI Engine",
                                    event = "Embeddings model file missing",
                                    status = StartupStatus.FAILED,
                                    error = errorMsg
                                )
                                ComponentHealthMonitor.updateComponentHealth(
                                    componentName = "Ensemble AI Engine",
                                    status = HealthStatus.FAILED,
                                    message = errorMsg
                                )
                                ModelDiagnostics.recordInitialization(
                                    modelType = ModelType.SENTENCE_EMBEDDINGS,
                                    success = false,
                                    error = errorMsg
                                )
                                return@withContext Result.failure(
                                    InitializationError.ModelNotFound("Embeddings model file missing")
                                )
                            }
                            
                            // Initialize required embeddings retriever
                            StartupDiagnosticCollector.logEvent(
                                component = "Ensemble AI Engine",
                                event = "Loading embeddings retriever",
                                status = StartupStatus.IN_PROGRESS
                            )
                            Timber.i("ðŸŽ¯ Initializing EmbeddingsRetriever from ${embeddingsFile.absolutePath}")
                            val embeddingsStartTime = System.currentTimeMillis()
                            embeddingsRetriever = EmbeddingsRetriever(embeddingsFile, knowledgeBase)
                            val embeddingsTime = System.currentTimeMillis() - embeddingsStartTime
                            
                            StartupDiagnosticCollector.logEvent(
                                component = "Ensemble AI Engine",
                                event = "Embeddings loaded",
                                status = StartupStatus.SUCCESS,
                                details = "Loaded in ${embeddingsTime}ms"
                            )
                            ModelDiagnostics.recordInitialization(
                                modelType = ModelType.SENTENCE_EMBEDDINGS,
                                success = true,
                                initTime = embeddingsTime
                            )
                            
                            // Initialize MobileBERT if available (optional - will use retrieval-only if missing)
                            if (mobileBertFile != null) {
                                try {
                                    StartupDiagnosticCollector.logEvent(
                                        component = "Ensemble AI Engine",
                                        event = "Loading MobileBERT Q&A",
                                        status = StartupStatus.IN_PROGRESS
                                    )
                                    Timber.i("ðŸŽ¯ Initializing MobileBERTQaAdapter from ${mobileBertFile.absolutePath}")
                                    val bertStartTime = System.currentTimeMillis()
                                    mobileBertQa = MobileBERTQaAdapter(mobileBertFile)
                                    val bertTime = System.currentTimeMillis() - bertStartTime
                                    
                                    Timber.i("ðŸŽ¯ MobileBERT Q&A available as fallback")
                                    StartupDiagnosticCollector.logEvent(
                                        component = "Ensemble AI Engine",
                                        event = "MobileBERT loaded",
                                        status = StartupStatus.SUCCESS,
                                        details = "Loaded in ${bertTime}ms"
                                    )
                                    ModelDiagnostics.recordInitialization(
                                        modelType = ModelType.MOBILEBERT_QA,
                                        success = true,
                                        initTime = bertTime
                                    )
                                } catch (bertError: Exception) {
                                    Timber.w(bertError, "ðŸŽ¯ MobileBERT initialization failed - will use retrieval-only mode")
                                    mobileBertQa = null
                                    StartupDiagnosticCollector.logEvent(
                                        component = "Ensemble AI Engine",
                                        event = "MobileBERT load failed",
                                        status = StartupStatus.WARNING,
                                        error = bertError.message
                                    )
                                    ModelDiagnostics.recordInitialization(
                                        modelType = ModelType.MOBILEBERT_QA,
                                        success = false,
                                        error = bertError.message,
                                        stackTrace = bertError.stackTraceToString()
                                    )
                                }
                            } else {
                                Timber.i("ðŸŽ¯ MobileBERT model not available - using retrieval-only mode")
                                mobileBertQa = null
                                StartupDiagnosticCollector.logEvent(
                                    component = "Ensemble AI Engine",
                                    event = "MobileBERT not available",
                                    status = StartupStatus.WARNING,
                                    details = "Using retrieval-only mode"
                                )
                            }
                            
                            // Initialize optional intent classifier if available
                            val intentFile = modelManager.getIntentClassifierFile()
                            if (intentFile != null) {
                                try {
                                    Timber.i("ðŸŽ¯ Initializing optional IntentClassifier from ${intentFile.absolutePath}")
                                    val intentStartTime = System.currentTimeMillis()
                                    intentClassifier = IntentClassifier(context, intentFile)
                                    val intentTime = System.currentTimeMillis() - intentStartTime
                                    Timber.i("ðŸŽ¯ Intent classification enabled")
                                    ModelDiagnostics.recordInitialization(
                                        modelType = ModelType.INTENT_CLASSIFIER,
                                        success = true,
                                        initTime = intentTime
                                    )
                                } catch (intentError: Exception) {
                                    Timber.w(intentError, "ðŸŽ¯ Intent classifier initialization failed")
                                    intentClassifier = null
                                    ModelDiagnostics.recordInitialization(
                                        modelType = ModelType.INTENT_CLASSIFIER,
                                        success = false,
                                        error = intentError.message,
                                        stackTrace = intentError.stackTraceToString()
                                    )
                                }
                            } else {
                                Timber.i("ðŸŽ¯ Intent classifier not available - running without intent classification")
                                intentClassifier = null
                            }
                            
                            modelState = ModelState.Ready
                            val modelsLoaded = buildString {
                                append("Embeddings retriever")
                                if (mobileBertQa != null) append(" + MobileBERT Q&A")
                                if (intentClassifier != null) append(" + Intent classifier")
                            }
                            Timber.i("ðŸŽ¯ EnsembleEngine ready - $modelsLoaded loaded and ready for inference")
                            
                            StartupDiagnosticCollector.logEvent(
                                component = "Ensemble AI Engine",
                                event = "Ready for inference",
                                status = StartupStatus.SUCCESS,
                                details = modelsLoaded
                            )
                            
                            // Update component health based on what loaded
                            val healthStatus = when {
                                mobileBertQa != null -> HealthStatus.HEALTHY
                                else -> HealthStatus.DEGRADED
                            }
                            val healthMessage = when {
                                mobileBertQa != null -> "All models loaded"
                                else -> "MobileBERT unavailable - retrieval only"
                            }
                            ComponentHealthMonitor.updateComponentHealth(
                                componentName = "Ensemble AI Engine",
                                status = healthStatus,
                                message = healthMessage,
                                details = mapOf("models_loaded" to modelsLoaded)
                            )
                            
                            return@withContext Result.success(Unit)
                            
                        } catch (loadError: Exception) {
                            // Clean up partial initialization
                            intentClassifier?.close()
                            embeddingsRetriever?.close()
                            mobileBertQa?.close()
                            intentClassifier = null
                            embeddingsRetriever = null
                            mobileBertQa = null
                            
                            // Note: Don't fail if only MobileBERT failed - embeddings retriever is sufficient
                            
                            modelState = ModelState.Error(
                                loadError.message ?: "Model initialization failed", 
                                recoverable = true
                            )
                            val error = InitializationError.LoadFailed(loadError)
                            Timber.e(error, "ðŸŽ¯ Ensemble initialization failed")
                            
                            StartupDiagnosticCollector.logEvent(
                                component = "Ensemble AI Engine",
                                event = "Initialization failed",
                                status = StartupStatus.FAILED,
                                error = loadError.message
                            )
                            ComponentHealthMonitor.updateComponentHealth(
                                componentName = "Ensemble AI Engine",
                                status = HealthStatus.FAILED,
                                message = loadError.message ?: "Initialization failed"
                            )
                            
                            return@withContext Result.failure(error)
                        }
                    }
                    
                    is ModelState.Ready -> {
                        // State says Ready but embeddings retriever is null - reinitialize
                        if (embeddingsRetriever == null) {
                            Timber.w("ðŸŽ¯ State is Ready but embeddings retriever is null - forcing reinitialization")
                            modelState = ModelState.Downloaded
                            return@withContext initialize()
                        } else {
                            Timber.d("ðŸŽ¯ Ensemble already ready")
                            return@withContext Result.success(Unit)
                        }
                    }
                    
                    else -> {
                        Timber.e("ðŸŽ¯ Unexpected model state during initialization: $modelState")
                        return@withContext Result.failure(InitializationError.LoaderNotImplemented())
                    }
                }
            } catch (e: Exception) {
                modelState = ModelState.Error(e.message ?: "Initialization failed", recoverable = false)
                Timber.e(e, "ðŸŽ¯ Fatal initialization error")
                
                StartupDiagnosticCollector.logEvent(
                    component = "Ensemble AI Engine",
                    event = "Fatal initialization error",
                    status = StartupStatus.FAILED,
                    error = e.message
                )
                ComponentHealthMonitor.updateComponentHealth(
                    componentName = "Ensemble AI Engine",
                    status = HealthStatus.FAILED,
                    message = e.message ?: "Fatal initialization error"
                )
                
                return@withContext Result.failure(InitializationError.LoadFailed(e))
            }
        }
    }
    
    /**
     * Generate answer for user question using ensemble approach
     * 
     * ## Orchestration Flow
     * 
     * 1. [OPTIONAL] Classify intent using IntentClassifier (if available)
     * 2. Search for answer using EmbeddingsRetriever
     * 3. If confidence > 0.75: return cached answer (10x faster)
     * 4. Otherwise: use MobileBERT to generate answer from context
     * 
     * Returns ExplanationResult.Unavailable when required models not loaded,
     * allowing PatternExplainer to use FallbackExplanations.
     */
    suspend fun generate(
        prompt: String,
        maxTokens: Int = 512
    ): ExplanationResult = withContext(Dispatchers.Default) {
        
        try {
            // Check if required models are loaded and ready (embeddings required, MobileBERT optional)
            if (modelState !is ModelState.Ready || 
                embeddingsRetriever == null) {
                
                val reason = when (modelState) {
                    is ModelState.NotDownloaded -> "Ensemble models not downloaded"
                    is ModelState.PartiallyDownloaded -> {
                        val state = modelState as ModelState.PartiallyDownloaded
                        "Only ${state.importedCount}/${state.totalRequired} required models imported"
                    }
                    is ModelState.Downloaded -> "Models not loaded into memory"
                    is ModelState.Loading -> "Models currently loading"
                    is ModelState.Error -> "Model error: ${(modelState as ModelState.Error).error}"
                    else -> "Models not ready"
                }
                
                return@withContext ExplanationResult.Unavailable(
                    reason = reason,
                    fallbackText = "Models not available. Using template response."
                )
            }
            
            val startTime = System.currentTimeMillis()
            val previousState = modelState
            modelState = ModelState.Generating
            
            try {
                Timber.d("ðŸŽ¯ Processing question: ${prompt.take(100)}...")
                
                // Step 1 (Optional): Classify intent if intent classifier is available
                val intentResult = if (intentClassifier != null) {
                    val result = intentClassifier!!.classify(prompt)
                    Timber.d("ðŸŽ¯ Intent: ${result.intent} (confidence: ${result.confidence})")
                    result
                } else {
                    Timber.d("ðŸŽ¯ Skipping intent classification (intent classifier not loaded)")
                    null
                }
                
                // Step 2: Try retrieval-based answer (fast path)
                val retrievalResult = embeddingsRetriever!!.retrieve(prompt)
                
                if (retrievalResult != null && retrievalResult.confidence >= RETRIEVAL_CONFIDENCE_THRESHOLD) {
                    // High confidence match - return cached answer instantly
                    modelState = previousState
                    val inferenceTime = System.currentTimeMillis() - startTime
                    
                    // Count tokens (approximate - count words)
                    val tokensGenerated = retrievalResult.answer.split("\\s+".toRegex()).size
                    
                    Timber.i("ðŸŽ¯ Retrieved cached answer in ${inferenceTime}ms (~$tokensGenerated tokens, confidence: ${retrievalResult.confidence})")
                    
                    return@withContext ExplanationResult.Success(
                        text = retrievalResult.answer,
                        tokensGenerated = tokensGenerated,
                        inferenceTimeMs = inferenceTime,
                        fromCache = true
                    )
                }
                
                // Step 3: Low confidence or no match - try MobileBERT if available, else use best retrieval
                if (mobileBertQa != null) {
                    try {
                        Timber.d("ðŸŽ¯ Retrieval confidence too low (${retrievalResult?.confidence ?: 0f}), using MobileBERT generation")
                        
                        // Build context for MobileBERT (use retrieved answer as context if available)
                        val context = if (retrievalResult != null) {
                            buildContextWithRetrieval(prompt, retrievalResult.answer, intentResult?.intent ?: "general")
                        } else {
                            buildContextFromIntent(prompt, intentResult?.intent ?: "general")
                        }
                        
                        val qaResult = mobileBertQa!!.answer(context, prompt)
                        
                        // Restore previous state
                        modelState = previousState
                        val inferenceTime = System.currentTimeMillis() - startTime
                        
                        // Count tokens (approximate - count words)
                        val tokensGenerated = qaResult.answer.split("\\s+".toRegex()).size
                        
                        Timber.i("ðŸŽ¯ Generated answer in ${inferenceTime}ms (~$tokensGenerated tokens, confidence: ${qaResult.confidence})")
                        
                        return@withContext ExplanationResult.Success(
                            text = qaResult.answer,
                            tokensGenerated = tokensGenerated,
                            inferenceTimeMs = inferenceTime,
                            fromCache = false
                        )
                    } catch (bertError: Exception) {
                        Timber.w(bertError, "ðŸŽ¯ MobileBERT failed, falling back to best retrieval match")
                        // Fall through to use retrieval result below
                    }
                }
                
                // MobileBERT unavailable or failed - use best retrieval match even if low confidence
                if (retrievalResult != null) {
                    modelState = previousState
                    val inferenceTime = System.currentTimeMillis() - startTime
                    val tokensGenerated = retrievalResult.answer.split("\\s+".toRegex()).size
                    
                    Timber.i("ðŸŽ¯ Using best retrieval match (confidence: ${retrievalResult.confidence}) in ${inferenceTime}ms")
                    
                    return@withContext ExplanationResult.Success(
                        text = retrievalResult.answer,
                        tokensGenerated = tokensGenerated,
                        inferenceTimeMs = inferenceTime,
                        fromCache = true
                    )
                } else {
                    // No retrieval result at all
                    modelState = previousState
                    Timber.w("ðŸŽ¯ No retrieval match found and MobileBERT unavailable")
                    
                    return@withContext ExplanationResult.Unavailable(
                        reason = "No matching answer found in knowledge base",
                        fallbackText = "I don't have information about that specific topic. Try asking about chart patterns, technical indicators, or trading strategies."
                    )
                }
                
            } catch (inferenceError: Exception) {
                // Restore previous state on error
                modelState = previousState
                Timber.e(inferenceError, "ðŸŽ¯ Ensemble inference failed")
                
                return@withContext ExplanationResult.Failure(
                    error = inferenceError.message ?: "Inference failed",
                    fallbackText = null
                )
            }
            
        } catch (e: Exception) {
            modelState = ModelState.Error(e.message ?: "Generation failed", recoverable = true)
            Timber.e(e, "ðŸŽ¯ Text generation failed")
            
            ExplanationResult.Failure(
                error = e.message ?: "Unknown error",
                fallbackText = null
            )
        }
    }
    
    /**
     * Build context for MobileBERT when retrieval found a low-confidence match
     * 
     * Uses the retrieved answer as additional context to improve generation quality
     */
    private fun buildContextWithRetrieval(
        question: String,
        retrievedAnswer: String,
        intent: String
    ): String {
        return """
            Related information: $retrievedAnswer
            
            Question: $question
            Intent: $intent
        """.trimIndent()
    }
    
    /**
     * Build context for MobileBERT when no retrieval match found
     * 
     * Uses intent to provide minimal context
     */
    private fun buildContextFromIntent(
        question: String,
        intent: String
    ): String {
        val intentContext = when (intent) {
            "pattern_explanation" -> "This is a question about chart patterns and technical analysis."
            "quantra_score" -> "This is a question about the Quantra Score metric for pattern quality."
            "trading_strategy" -> "This is a question about trading strategies and market analysis."
            "indicator" -> "This is a question about technical indicators."
            "validation" -> "This is a question about pattern validation and confirmation."
            else -> "This is a general question about trading and markets."
        }
        
        return """
            $intentContext
            
            Question: $question
        """.trimIndent()
    }
    
    /**
     * Get list of missing REQUIRED models for PartiallyDownloaded state
     * 
     * Only reports missing required models (embeddings + mobilebert).
     * Intent classifier is optional and not reported as missing.
     */
    private fun getMissingModels(state: ModelState.PartiallyDownloaded): List<String> {
        val requiredModels = setOf(
            com.lamontlabs.quantravision.intelligence.llm.ModelType.SENTENCE_EMBEDDINGS,
            com.lamontlabs.quantravision.intelligence.llm.ModelType.MOBILEBERT_QA
        )
        
        val missing = requiredModels - state.importedModels
        
        return missing.map { modelType ->
            when (modelType) {
                com.lamontlabs.quantravision.intelligence.llm.ModelType.SENTENCE_EMBEDDINGS -> 
                    "sentence_embeddings.tflite"
                com.lamontlabs.quantravision.intelligence.llm.ModelType.MOBILEBERT_QA -> 
                    "mobilebert_qa_squad.tflite"
                else -> ""
            }
        }.filter { it.isNotEmpty() }
    }
    
    /**
     * Get current model state
     */
    fun getState(): ModelState = modelState
    
    /**
     * Check if required ensemble models are loaded and ready
     * 
     * Returns true when modelState is Ready, indicating that required models
     * (embeddings + MobileBERT) are loaded in memory and ready to generate answers.
     * 
     * Intent classifier is optional - engine is ready with or without it.
     * 
     * Returns false when:
     * - No models downloaded (NotDownloaded)
     * - Missing required models (PartiallyDownloaded)
     * - Models downloaded but not loaded (Downloaded)
     * - Models are loading (Loading)
     * - Models encountered an error (Error)
     */
    fun isReady(): Boolean {
        return modelState is ModelState.Ready && 
               embeddingsRetriever != null && 
               mobileBertQa != null
    }
    
    /**
     * Unload ensemble models from memory to free resources
     * 
     * Useful for memory management when models are not actively needed.
     * Call initialize() again to reload the models.
     */
    fun unload() {
        intentClassifier?.close()
        embeddingsRetriever?.close()
        mobileBertQa?.close()
        
        intentClassifier = null
        embeddingsRetriever = null
        mobileBertQa = null
        
        // Update state to Downloaded if model files still exist
        modelState = modelManager.getModelState()
        
        Timber.i("ðŸŽ¯ Ensemble models unloaded from memory - State: $modelState")
    }
}
