# Model Optimization Guide - YOLOv8 Quantization & Deployment

**QuantraVision AI Enhancement - Offline Training Instructions**  
**Date:** October 31, 2025

---

## ðŸŽ¯ Objective

Convert the existing YOLOv8 PyTorch model (`stockmarket-pattern-yolov8.pt`, 84 MB) into optimized TFLite models for on-device inference with:

- **74% smaller size** (84 MB â†’ 22 MB INT8 quantized)
- **60% faster inference** (20ms â†’ 8ms)
- **+3% accuracy** (93.2% â†’ 96%+ mAP@0.5)
- **GPU acceleration** support (Pixel, Samsung flagship)
- **NNAPI support** (MediaTek, Qualcomm mid-range)

---

## ðŸ“‹ Prerequisites

### Software Requirements

```bash
# Python environment
python >= 3.9
pytorch >= 2.0
ultralytics >= 8.0.200
tensorflow >= 2.17.0
onnx >= 1.14.0
nncf >= 2.6.0

# Install dependencies
pip install ultralytics tensorflow onnx nncf albumentations
```

### Hardware Requirements

- **GPU:** NVIDIA GPU with CUDA (recommended for faster training)
- **RAM:** 16 GB+ (for training and quantization)
- **Disk:** 10 GB free space

---

## ðŸš€ Step-by-Step Optimization Pipeline

### Phase 1: Export to ONNX (Baseline)

```bash
# Navigate to model directory
cd app/src/main/assets/models/

# Export YOLOv8 to ONNX format
yolo export model=stockmarket-pattern-yolov8.pt format=onnx imgsz=640

# Verify ONNX model
python -c "import onnx; model = onnx.load('stockmarket-pattern-yolov8.onnx'); onnx.checker.check_model(model)"
```

**Expected Output:**
- `stockmarket-pattern-yolov8.onnx` (~84 MB, FP32)

---

### Phase 2: Quantization-Aware Training (QAT)

**Why QAT?** Maintains accuracy better than post-training quantization.

```python
# train_quantized.py
from ultralytics import YOLO
import torch

# Load original model
model = YOLO('stockmarket-pattern-yolov8.pt')

# Configure quantization settings
qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
model.model.qconfig = qconfig

# Prepare for QAT
torch.quantization.prepare_qat(model.model, inplace=True)

# Fine-tune for 20 epochs with quantization
model.train(
    data='stockmarket-patterns.yaml',  # Your dataset config
    epochs=20,
    imgsz=512,  # Reduced from 640
    batch=16,
    device='cuda',
    optimizer='AdamW',
    lr0=0.0001,  # Lower learning rate for fine-tuning
    weight_decay=0.0005,
    augment=True
)

# Convert to quantized model
torch.quantization.convert(model.model, inplace=True)

# Save quantized PyTorch model
model.save('stockmarket-pattern-yolov8-qat.pt')
```

**Expected Output:**
- `stockmarket-pattern-yolov8-qat.pt` (~42 MB, INT8-aware)
- Accuracy: 93.5%+ mAP@0.5 (minimal drop from 93.2%)

---

### Phase 3: INT8 Quantization with TFLite Converter

```python
# convert_to_tflite_int8.py
import tensorflow as tf
from ultralytics import YOLO
import numpy as np

# Load QAT model
model = YOLO('stockmarket-pattern-yolov8-qat.pt')

# Export to TensorFlow SavedModel first
model.export(format='saved_model', imgsz=512)

# Representative dataset for calibration
def representative_dataset():
    """
    Provide 100 representative images for INT8 calibration
    """
    import glob
    from PIL import Image
    
    images = glob.glob('calibration_images/*.png')[:100]
    
    for img_path in images:
        img = Image.open(img_path).resize((512, 512))
        img_array = np.array(img, dtype=np.float32) / 255.0
        img_array = np.expand_dims(img_array, axis=0)
        yield [img_array]

# Convert to INT8 TFLite
converter = tf.lite.TFLiteConverter.from_saved_model('yolov8_saved_model')

# Optimization settings
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset

# INT8 quantization
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
    tf.lite.OpsSet.SELECT_TF_OPS  # Fallback for unsupported ops
]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

# Convert
tflite_model = converter.convert()

# Save INT8 model
with open('yolov8_int8_optimized.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"INT8 model size: {len(tflite_model) / 1024 / 1024:.2f} MB")
```

**Expected Output:**
- `yolov8_int8_optimized.tflite` (~22 MB)
- Accuracy: 92.5%+ mAP@0.5

---

### Phase 4: FP16 Hybrid Quantization (Fallback)

```python
# convert_to_tflite_fp16.py
import tensorflow as tf
from ultralytics import YOLO

model = YOLO('stockmarket-pattern-yolov8-qat.pt')
model.export(format='saved_model', imgsz=512)

converter = tf.lite.TFLiteConverter.from_saved_model('yolov8_saved_model')

# FP16 quantization (larger but more accurate)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

# Convert
tflite_model = converter.convert()

# Save FP16 model
with open('yolov8_fp16_hybrid.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"FP16 model size: {len(tflite_model) / 1024 / 1024:.2f} MB")
```

**Expected Output:**
- `yolov8_fp16_hybrid.tflite` (~42 MB)
- Accuracy: 93.8%+ mAP@0.5 (better than INT8)

---

### Phase 5: Model Pruning (Advanced - Optional)

```python
# prune_model.py
import nncf
from nncf import NNCFConfig, create_compressed_model
from ultralytics import YOLO

# Load model
model = YOLO('stockmarket-pattern-yolov8.pt')

# NNCF compression config (35% sparsity)
nncf_config = {
    "input_info": {"sample_size": [1, 3, 512, 512]},
    "compression": {
        "algorithm": "magnitude_sparsity",
        "params": {
            "schedule": "exponential",
            "sparsity_init": 0.05,
            "sparsity_target": 0.35,  # 35% pruning
            "sparsity_target_epoch": 50
        }
    }
}

# Create compressed model
compression_ctrl, compressed_model = create_compressed_model(
    model.model,
    nncf_config
)

# Fine-tune pruned model
model.train(
    data='stockmarket-patterns.yaml',
    epochs=50,
    imgsz=512,
    batch=16,
    device='cuda'
)

# Export pruned model
compression_ctrl.export_model('stockmarket-pattern-yolov8-pruned.pt')
```

**Expected Output:**
- `stockmarket-pattern-yolov8-pruned.pt` (~28 MB)
- Accuracy: 93.8%+ mAP@0.5
- Inference: 6ms (vs. 8ms unpruned)

---

### Phase 6: Knowledge Distillation (Advanced - Optional)

```python
# distill_model.py
from ultralytics import YOLO
import torch
import torch.nn as nn

# Teacher: Original large model
teacher = YOLO('stockmarket-pattern-yolov8.pt')

# Student: Smaller model (YOLOv8n or custom)
student = YOLO('yolov8n.pt')

# Distillation loss
def distillation_loss(student_output, teacher_output, labels, temperature=3.0, alpha=0.5):
    """
    Combine task loss with distillation loss
    """
    # Task loss (ground truth)
    task_loss = student.loss(student_output, labels)
    
    # Distillation loss (soft targets from teacher)
    soft_targets = nn.functional.softmax(teacher_output / temperature, dim=-1)
    soft_student = nn.functional.log_softmax(student_output / temperature, dim=-1)
    distill_loss = nn.functional.kl_div(soft_student, soft_targets, reduction='batchmean')
    
    # Combined loss
    return alpha * task_loss + (1 - alpha) * distill_loss * (temperature ** 2)

# Training loop with distillation
for epoch in range(100):
    for batch in dataloader:
        images, labels = batch
        
        # Get teacher predictions (no grad)
        with torch.no_grad():
            teacher_output = teacher(images)
        
        # Get student predictions
        student_output = student(images)
        
        # Compute distillation loss
        loss = distillation_loss(student_output, teacher_output, labels)
        
        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Save distilled model
student.save('stockmarket-pattern-yolov8-distilled.pt')
```

**Expected Output:**
- `stockmarket-pattern-yolov8-distilled.pt` (~18 MB)
- Accuracy: 93.8%+ mAP@0.5 (improved via teacher guidance)
- Inference: 5ms

---

## ðŸ“Š Validation & Benchmarking

### Accuracy Validation

```python
# validate_model.py
from ultralytics import YOLO

# Load model
model = YOLO('yolov8_int8_optimized.tflite', task='detect')

# Validate on test set
results = model.val(
    data='stockmarket-patterns.yaml',
    split='test',
    imgsz=512,
    batch=1,
    device='cpu'  # TFLite uses CPU/GPU delegate
)

print(f"mAP@0.5: {results.box.map50:.4f}")
print(f"mAP@0.5:0.95: {results.box.map:.4f}")
print(f"Precision: {results.box.mp:.4f}")
print(f"Recall: {results.box.mr:.4f}")
```

**Target Metrics:**
- mAP@0.5: â‰¥96%
- mAP@0.5:0.95: â‰¥75%
- Precision: â‰¥94%
- Recall: â‰¥92%

---

### Speed Benchmark

```python
# benchmark_speed.py
import tensorflow as tf
import numpy as np
import time

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path='yolov8_int8_optimized.tflite')
interpreter.allocate_tensors()

# Get input/output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Benchmark
iterations = 100
test_image = np.random.randint(0, 256, (1, 512, 512, 3), dtype=np.uint8)

# Warmup
for _ in range(10):
    interpreter.set_tensor(input_details[0]['index'], test_image)
    interpreter.invoke()

# Benchmark
start_time = time.time()
for _ in range(iterations):
    interpreter.set_tensor(input_details[0]['index'], test_image)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[0]['index'])

avg_time_ms = (time.time() - start_time) / iterations * 1000

print(f"Average inference time: {avg_time_ms:.2f}ms")
print(f"FPS: {1000 / avg_time_ms:.2f}")
```

**Target Speed:**
- CPU: â‰¤12ms
- GPU delegate: â‰¤8ms
- NNAPI: â‰¤10ms

---

## ðŸ“¦ Deployment to Android

### Step 1: Copy Optimized Models to Assets

```bash
# Copy to Android assets directory
cp yolov8_int8_optimized.tflite app/src/main/assets/models/
cp yolov8_fp16_hybrid.tflite app/src/main/assets/models/

# Verify sizes
ls -lh app/src/main/assets/models/
```

### Step 2: Update Model Paths in Code

```kotlin
// In OptimizedModelLoader.kt
enum class ModelType(val path: String) {
    INT8_QUANTIZED("models/yolov8_int8_optimized.tflite"),    // 22 MB - Primary
    FP16_HYBRID("models/yolov8_fp16_hybrid.tflite"),          // 42 MB - Fallback
    FP32_FULL("models/stockmarket-pattern-yolov8.pt")         // 84 MB - Legacy
}
```

### Step 3: Test on Android Device

```bash
# Build and install APK
./gradlew assembleDebug
adb install -r app/build/outputs/apk/debug/app-debug.apk

# Run instrumented tests
./gradlew connectedAndroidTest

# Monitor performance
adb shell "am instrument -w -r -e class com.lamontlabs.quantravision.ml.PerformanceBenchmarkTest \
  com.lamontlabs.quantravision.test/androidx.test.runner.AndroidJUnitRunner"
```

---

## ðŸ”§ Troubleshooting

### Issue: Accuracy Drop >5%

**Solution:** Use FP16 hybrid quantization or knowledge distillation

```python
# Use more calibration data (500+ images instead of 100)
def representative_dataset():
    images = glob.glob('calibration_images/*.png')[:500]
    # ...
```

### Issue: Model Size Still Large

**Solution:** Apply structured pruning before quantization

```bash
# Combine pruning + quantization
python prune_model.py  # 35% sparsity
python convert_to_tflite_int8.py  # Then quantize
```

### Issue: GPU Delegate Not Working

**Solution:** Check TFLite compatibility

```kotlin
// Add error handling in OptimizedModelLoader
try {
    options.addDelegate(GpuDelegate(gpuOptions))
} catch (e: IllegalArgumentException) {
    Timber.w("GPU delegate not supported, falling back to NNAPI")
    configureNNAPI(options)
}
```

---

## ðŸ“ˆ Expected Results Summary

| Model Variant | Size | Inference (GPU) | Inference (CPU) | Accuracy (mAP@0.5) |
|---------------|------|-----------------|-----------------|-------------------|
| **Original PyTorch** | 84 MB | N/A | N/A | 93.2% |
| **INT8 Quantized** | 22 MB | **5ms** | 12ms | 92.5% |
| **FP16 Hybrid** | 42 MB | **6ms** | 15ms | 93.8% |
| **Pruned + INT8** | 18 MB | **4ms** | 10ms | 93.8% |
| **Distilled** | 18 MB | **4ms** | 9ms | 94.2% |

**Recommended:** INT8 Quantized (best balance of size/speed/accuracy)

---

## ðŸŽ¯ Success Criteria

âœ… **Model size:** â‰¤22 MB (INT8)  
âœ… **Inference speed:** â‰¤8ms (GPU), â‰¤12ms (CPU)  
âœ… **Accuracy:** â‰¥92.5% mAP@0.5 (Â±0.7% from baseline)  
âœ… **Battery:** <1.2W sustained power draw  
âœ… **Compatibility:** Android 7.0+ (API 24+)  

---

## ðŸ“ž Next Steps

1. **Run quantization pipeline** on local machine with GPU
2. **Validate accuracy** on test set (target: â‰¥96%)
3. **Deploy models** to `app/src/main/assets/models/`
4. **Run benchmarks** on physical Android device
5. **Optimize further** if needed (pruning, distillation)

---

**Â© 2025 Lamont Labs. Model Optimization Guide â€” Confidential.**

**Making QuantraVision the fastest AI detector on Android.** ðŸš€
